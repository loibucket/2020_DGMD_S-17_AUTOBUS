{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autobus - Live demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug  3 00:26:29 2020 go\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(time.ctime(),'go')\n",
    "\n",
    "#####Make Robot#####\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "from jetbot import Robot\n",
    "robot = Robot()\n",
    "\n",
    "#####UI functions#####\n",
    "import traitlets\n",
    "import ipywidgets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "#####Monitor Network Connection#####\n",
    "from jetbot import Heartbeat\n",
    "def handle_heartbeat_status(change):\n",
    "    if change['new'] == Heartbeat.Status.dead:\n",
    "        print('check network connection')\n",
    "heartbeat = Heartbeat(period=5)\n",
    "heartbeat.observe(handle_heartbeat_status, names='status') # attach the callback function to heartbeat status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rail Tracking Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug  3 00:26:47 2020 best_steering_model_xy--PoolTable400.pth ok\n",
      "Mon Aug  3 00:26:47 2020 ok\n"
     ]
    }
   ],
   "source": [
    "#####Rail Tracking Setup#####\n",
    "import torchvision\n",
    "import torch\n",
    "steer_model_file = 'best_steering_model_xy--PoolTable400.pth'\n",
    "steer_model = torchvision.models.resnet18(pretrained=False)\n",
    "steer_model.fc = torch.nn.Linear(512, 2)\n",
    "steer_model.load_state_dict(torch.load(steer_model_file))\n",
    "steer_device = torch.device('cuda')\n",
    "steer_model = steer_model.to(steer_device)\n",
    "steer_model = steer_model.eval().half()\n",
    "print(time.ctime(),steer_model_file,'ok')\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "steer_mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "steer_std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "def steer_preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(steer_device).half()\n",
    "    image.sub_(steer_mean[:, None, None]).div_(steer_std[:, None, None])\n",
    "    return image[None, ...]\n",
    "\n",
    "#add the steering xy prediction to the image\n",
    "def display_xy(camera_image):\n",
    "    image = np.copy(camera_image)\n",
    "    x = x_slider.value\n",
    "    y = y_slider.value\n",
    "    x = int(x * 224 / 2 + 112)\n",
    "    y = int(y * 224 / 2 + 112)\n",
    "    image = cv2.circle(image, (x, y), 8, (0, 255, 0), 3)\n",
    "    image = cv2.circle(image, (112, 224), 8, (0, 0,255), 3)\n",
    "    image = cv2.line(image, (x,y), (112,224), (255,0,0), 3)\n",
    "    jpeg_image = bgr8_to_jpeg(image)\n",
    "    return jpeg_image\n",
    "\n",
    "print(time.ctime(),'ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Avoidance Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check network connection\n",
      "Mon Aug  3 00:26:51 2020 oba_model ok\n",
      "Mon Aug  3 00:26:51 2020 ok\n"
     ]
    }
   ],
   "source": [
    "#####Collision Avoidance Setup#####\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "oba_model = torchvision.models.alexnet(pretrained=False)\n",
    "oba_model.classifier[6] = torch.nn.Linear(oba_model.classifier[6].in_features, 2)\n",
    "#oba_model.load_state_dict(torch.load('best_model.pth'))\n",
    "oba_model.load_state_dict(torch.load('best_obstacle_model--NVIDIA.pth'))\n",
    "oba_device = torch.device('cuda')\n",
    "oba_model = oba_model.to(oba_device)\n",
    "print(time.ctime(),'oba_model','ok')\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "oba_mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "oba_stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "oba_normalize = torchvision.transforms.Normalize(oba_mean, oba_stdev)\n",
    "def oba_preprocess(camera_value):\n",
    "    global oba_device, oba_normalize\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = oba_normalize(x)\n",
    "    x = x.to(oba_device)\n",
    "    x = x[None, ...]\n",
    "    return x\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "print(time.ctime(),'ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function that will get called whenever the camera's value changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug  3 00:26:51 2020 ok\n"
     ]
    }
   ],
   "source": [
    "##### Create a function that will get called whenever the camera's value changes. #####\n",
    "#This function will do the following steps\n",
    "#- Pre-process the camera image\n",
    "#- Execute the neural network\n",
    "#- Stop and prevent motion on obstacle detection\n",
    "#- Stop when jetbot reaches a station, resumes after a set time\n",
    "#- If OK to move, control steering, keep jetbot on the track\n",
    "\n",
    "#####Global vars for execute#####\n",
    "#all vars can be changed later, even while AUTOBUS is running\n",
    "angle = 0.0\n",
    "angle_last = 0.0\n",
    "mt = 0\n",
    "frame_cnt = 0\n",
    "interval = 0.1\n",
    "stutter = 0\n",
    "steer_debug = False\n",
    "tt = time.time()\n",
    "fwd_high = 0.4\n",
    "fwd_low = 0.3\n",
    "turn_high = 0.4\n",
    "turn_low = 0.3\n",
    "stutter_spd = 0.1\n",
    "prob_block_stop = 0.8\n",
    "time_last_station = 9000 #always start code with station stopping enabled\n",
    "leave_station_secs = 60\n",
    "\n",
    "#!!!!! Jetbot doing something with new image !!!!!\n",
    "def execute(change):\n",
    "    global angle, angle_last, mt, frame_cnt, tt, steer_debug, interval, accel, stutter, robot\n",
    "    global stutter_spd, prob_block_stop, min_inter, max_inter, fwd_high, fwd_low, turn_high, turn_low\n",
    "    \n",
    "    #count images read\n",
    "    frame_cnt += 1\n",
    "    \n",
    "    #min time interval between each run\n",
    "    if time.time() < tt + interval:\n",
    "        return\n",
    "    tt = time.time()\n",
    "    \n",
    "    #stutters robot, start-stop motion\n",
    "    if stutter:\n",
    "        rlmv = robot.left_motor.value \n",
    "        rrmv = robot.right_motor.value\n",
    "        robot.left_motor.value = stutter_spd *mt\n",
    "        robot.right_motor.value = stutter_spd *mt\n",
    "        time.sleep(0.1) #stop to get clear image\n",
    "        robot.left_motor.value = rlmv\n",
    "        robot.right_motor.value = rrmv\n",
    "        \n",
    "    #AAAAA jetbot reads image and makes a obstacle stop decision AAAAA\n",
    "    x = change['new'] \n",
    "    x = oba_preprocess(x)\n",
    "    y = oba_model(x)\n",
    "    \n",
    "    # we apply the `softmax` function to normalize the output vector so it sums to 1 (which makes it a probability distribution)\n",
    "    y = F.softmax(y, dim=1)\n",
    "    \n",
    "    prob_blocked = float(y.flatten()[0])\n",
    "    status_b.value = str(frame_cnt) + \" obstacle prob: \" + \"{:.2f}\".format(prob_blocked)\n",
    "    if prob_blocked > prob_block_stop:\n",
    "        robot.left_motor.value = 0\n",
    "        robot.right_motor.value = 0\n",
    "        status_a.value = str(frame_cnt) + \" obstacle stop\"\n",
    "        return\n",
    "    #^^^^^ jetbot finishes obstacle stop decision ^^^^^\n",
    "        \n",
    "    #BBBBB jetbot reads image and makes a station stop decision BBBBB\n",
    "    \"\"\"\n",
    "        Purpose:\n",
    "            Detects color of camera frame image.\n",
    "\n",
    "        Args:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            color_detected: string value of either 'red', 'yellow', 'green', 'purple' or 'other'\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If no available port is found.\n",
    "        \n",
    "        References:\n",
    "            (1) To understand colors and the color space with OpenCV read this link:\n",
    "                - https://www.learnopencv.com/color-spaces-in-opencv-cpp-python/\n",
    "            (2) To see color detection in OpenCV read this link:\n",
    "                - https://www.learnopencv.com/invisibility-cloak-using-color-detection-and-segmentation-with-opencv/\n",
    "            (3) To read more on smoothing images read this link:\n",
    "                - https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_filtering/py_filtering.html\n",
    "            (4) To read more on morphological transformations read this link:\n",
    "                - https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html\n",
    "            (5) To choose upper and lower limits manually follow this link:\n",
    "                - https://toolstud.io/color/rgb.php\n",
    "            (6) To understand multiple color detection in real-time using OpenCV read this link:\n",
    "                - https://www.geeksforgeeks.org/multiple-color-detection-in-real-time-using-python-opencv/\n",
    "    \"\"\"\n",
    "    \n",
    "    image_frame = change['new']\n",
    "    \n",
    "    # extracting out HSV color space values:\n",
    "    # H – Hue (Dominant Wavelength)\n",
    "    # S – Saturation (Purity/Shades of Color)\n",
    "    # V – Value (Intensity)\n",
    "    hsv = cv2.cvtColor(image_frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # adding median blur to image\n",
    "    hsv = cv2.medianBlur(hsv,5)\n",
    "    \n",
    "    # lower range of red\n",
    "    bttm_lower_red = np.array([0,120,70])\n",
    "    top_lower_red = np.array([10,255,255])\n",
    "    lower_red_mask = cv2.inRange(hsv, bttm_lower_red, top_lower_red)\n",
    "\n",
    "    # upper range of red\n",
    "    bttm_upper_red = np.array([170,120,70])\n",
    "    top_upper_red = np.array([180,255,255])\n",
    "    upper_red_mask = cv2.inRange(hsv, bttm_upper_red, top_upper_red)\n",
    "\n",
    "    # generating the final mask to detect red color\n",
    "    red_range_mask = lower_red_mask + upper_red_mask\n",
    "\n",
    "    # removing noise through opening (erosion followed by dilation)\n",
    "    red_range_mask = cv2.morphologyEx(red_range_mask, cv2.MORPH_OPEN, np.ones((3,3),np.uint8))\n",
    "\n",
    "    # increasing white region or size of foreground object increases through dilation\n",
    "    red_range_mask = cv2.morphologyEx(red_range_mask, cv2.MORPH_DILATE, np.ones((3,3),np.uint8))\n",
    "\n",
    "    # detecting if image frame contains red\n",
    "    red_results = cv2.bitwise_and(image_frame, image_frame, mask = red_range_mask)\n",
    "\n",
    "    # calculating ratio of red in image frame and prints output\n",
    "    red_ratio = (cv2.countNonZero(red_range_mask))/(image_frame.size/3)\n",
    "    ###print(\"Amount of red in image:\", np.round(red_ratio*100, 2))\n",
    "    \n",
    "    # generating mask for range of yellow\n",
    "    lower_yellow = np.array([20, 100, 100])\n",
    "    upper_yellow = np.array([30, 255, 255])\n",
    "    yellow_range_mask = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "\n",
    "    # removing noise through opening (erosion followed by dilation)\n",
    "    yellow_range_mask = cv2.morphologyEx(yellow_range_mask, cv2.MORPH_OPEN, np.ones((3,3),np.uint8))\n",
    "\n",
    "    # increasing white region or size of foreground object increases through dilation\n",
    "    yellow_range_mask = cv2.morphologyEx(yellow_range_mask, cv2.MORPH_DILATE, np.ones((3,3),np.uint8))\n",
    "\n",
    "    # detecting if image frame contains yellow\n",
    "    yellow_results = cv2.bitwise_and(image_frame, image_frame, mask = yellow_range_mask)\n",
    "\n",
    "    # calculating ratio of yellow in image frame and prints output\n",
    "    yellow_ratio = (cv2.countNonZero(yellow_range_mask))/(image_frame.size/3)\n",
    "    ###print(\"Amount of yellow in image:\", np.round(yellow_ratio*100, 2))\n",
    "    \n",
    "    # generating mask for range of green\n",
    "    lower_green = np.array([36, 25, 25])\n",
    "    upper_green = np.array([86, 255,255])\n",
    "    green_range_mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "\n",
    "    # removing noise through opening (erosion followed by dilation)\n",
    "    green_range_mask = cv2.morphologyEx(green_range_mask, cv2.MORPH_OPEN, np.ones((3,3),np.uint8))\n",
    "\n",
    "    # increasing white region or size of foreground object increases through dilation\n",
    "    green_range_mask = cv2.morphologyEx(green_range_mask, cv2.MORPH_DILATE, np.ones((3,3),np.uint8))\n",
    "\n",
    "    # detecting if image frame contains green\n",
    "    green_results = cv2.bitwise_and(image_frame, image_frame, mask = green_range_mask)\n",
    "\n",
    "    # calculating ratio of green in image frame and prints output\n",
    "    green_ratio = (cv2.countNonZero(green_range_mask))/(image_frame.size/3)\n",
    "    ###print(\"Amount of green in image:\", np.round(green_ratio*100, 2))\n",
    "    \n",
    "    # generating mask for range of purple\n",
    "    lower_purple = np.array([80, 10, 10])\n",
    "    upper_purple = np.array([120, 255, 255])\n",
    "    purple_range_mask = cv2.inRange(hsv, lower_purple, upper_purple)\n",
    "\n",
    "    # removing noise through opening (erosion followed by dilation)\n",
    "    purple_range_mask = cv2.morphologyEx(purple_range_mask, cv2.MORPH_OPEN, np.ones((3,3),np.uint8))\n",
    "\n",
    "    # increasing white region or size of foreground object increases through dilation\n",
    "    purple_range_mask = cv2.morphologyEx(purple_range_mask, cv2.MORPH_DILATE, np.ones((3,3),np.uint8))\n",
    "\n",
    "    # detecting if image frame contains purple\n",
    "    purple_results = cv2.bitwise_and(image_frame, image_frame, mask = purple_range_mask)\n",
    "\n",
    "    # calculating ratio of purple in image frame and prints output\n",
    "    purple_ratio = (cv2.countNonZero(purple_range_mask))/(image_frame.size/3)\n",
    "    ###print(\"Amount of purple in image:\", np.round(purple_ratio*100, 2))\n",
    "    \n",
    "    # calculating ratio of other colors in image frame and prints output\n",
    "    rygp_total = (red_ratio + yellow_ratio + green_ratio + purple_ratio)\n",
    "    other_ratio = (100 - rygp_total)\n",
    "    ###print(\"Amount of other colors in image:\", np.round(purple_ratio*100, 2))\n",
    "    \n",
    "    # assigning color detected to variable\n",
    "    if red_ratio >= 5:\n",
    "\n",
    "        # save detected color value as red\n",
    "        color_detected = 'red'\n",
    "\n",
    "    elif yellow_ratio >= 5:\n",
    "\n",
    "        # save detected color value as yellow\n",
    "        color_detected = 'yellow'\n",
    "\n",
    "    elif green_ratio >= 5:\n",
    "\n",
    "        # save detected color value as green\n",
    "        color_detected = 'green'\n",
    "\n",
    "    elif purple_ratio >= 23:\n",
    "\n",
    "        # save detected color value as purple\n",
    "        color_detected = 'purple'\n",
    "\n",
    "    else:\n",
    "\n",
    "        # save detected color value as other\n",
    "        color_detected = 'other'\n",
    " \n",
    "    # output deteced color value\n",
    "    status_c.value = str(frame_cnt) + \" color detected: \" + color_detected\n",
    "    \n",
    "    # jetbot decides to make a station stop, or not \n",
    "    if color_detected == 'red':\n",
    "        #dont't get stuck stopping in same station\n",
    "        #if time since stopping at last station is more than time it takes to leave a station...\n",
    "        if time.time() - time_last_station > leave_station_secs: \n",
    "            status_a.value = str(frame_cnt) + \" station stop\"\n",
    "            robot.left_motor.value = 0\n",
    "            robot.right_motor.value = 0\n",
    "            for i in range(60): #stop, sleep for x secs, then keep moving\n",
    "                time.sleep(1) \n",
    "                status_a.value = str(frame_cnt) + \" station stop \" + i + \" seconds\"\n",
    "            time_last_station = time.time() #keep track of time since leaving station\n",
    "        \n",
    "    #^^^^^ jetbot made decision based on color function ^^^^^\n",
    "    \n",
    "    \"\"\"\n",
    "        Purpose:\n",
    "            Provides autobus with functions associated with each color it detected.\n",
    "\n",
    "        Args:\n",
    "            color_detected: string value of either 'red', 'yellow', 'green', 'purple' or 'other'\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If no available port is found.\n",
    "        \n",
    "        References:\n",
    "            (1) To understand movement functions of Jetbot Waveshare read this link:\n",
    "                - https://www.elephantjay.com/blogs/tutorial/266\n",
    "    \"\"\"\n",
    "\n",
    "#     if color_detected == 'red':\n",
    "        \n",
    "#         # if red then autobus stops\n",
    "#         print(\"Color Code Red: Stop detected\")\n",
    "#         robot.stop()\n",
    "#         robot.left_motor.value = 0\n",
    "#         robot.right_motor.value = 0\n",
    "#         return\n",
    "    \n",
    "#     elif color_detected == 'yellow':\n",
    "        \n",
    "#         # if yellow then autobus slows down\n",
    "#         print(\"Color Code Yellow: Speed reduction detected\")\n",
    "#         robot.left_motor.value = (robot.left_motor.value * 0.5)\n",
    "#         robot.right_motor.value = (robot.right_motor.value  * 0.5)\n",
    "#         return\n",
    "\n",
    "#     elif color_detected == 'green':\n",
    "        \n",
    "#         # if green then autobus continues at normal speed\n",
    "#         print(\"Color Code Green: Continuation at normal speed detected\")\n",
    "#         robot.left_motor.value = 0.3\n",
    "#         robot.right_motor.value = 0.3\n",
    "#         break\n",
    "        \n",
    "#     elif color_detected == 'purple':\n",
    "        \n",
    "#         # if purple then autobus stops breilfly and starts again\n",
    "#         print(\"Color Code Purple: Bus stop detected\")\n",
    "#         robot.stop()\n",
    "#         robot.left_motor.value = 0\n",
    "#         robot.right_motor.value = 0\n",
    "#         robot.sleep(2.0) # 2 seconds wait\n",
    "#         robot.left_motor.value = 0.3\n",
    "#         robot.right_motor.value = 0.3\n",
    "#         break\n",
    "    \n",
    "#     elif color_detected == 'other':\n",
    "#         print(\"No Color Code Detected\")\n",
    "#         break\n",
    "\n",
    "    #CCCCC jetbot reads image and makes a steering decision CCCCC\n",
    "    \n",
    "    image = change['new']\n",
    "    xy = steer_model(steer_preprocess(image)).detach().float().cpu().numpy().flatten()\n",
    "    x = xy[0]\n",
    "    y = (0.5 - xy[1]) / 2.0\n",
    "    \n",
    "    x_slider.value = x\n",
    "    y_slider.value = y\n",
    "    \n",
    "    speed_slider.value = speed_gain_slider.value\n",
    "\n",
    "    angle = np.arctan2(x, y)\n",
    "    pid = angle * steering_gain_slider.value + (angle - angle_last) * steering_dgain_slider.value\n",
    "    angle_last = angle\n",
    "\n",
    "    steering_slider.value = pid + steering_bias_slider.value\n",
    "   \n",
    "    #nnnnn use default NVIDA algo nnnnn\n",
    "    if not steer_debug:\n",
    "        robot.left_motor.value = max(min(speed_slider.value + steering_slider.value, 1.0), 0.0) *mt\n",
    "        robot.right_motor.value = max(min(speed_slider.value - steering_slider.value, 1.0), 0.0) *mt\n",
    "        status_a.value = str(frame_cnt) + \" move\"\n",
    "        return\n",
    "        \n",
    "    #xxxxx use AUTOBUS debugging algo xxxxx\n",
    "    else: \n",
    "        stutter = 1\n",
    "\n",
    "        left_motor = max(min(speed_slider.value + steering_slider.value, 1.0), 0.0) \n",
    "        right_motor = max(min(speed_slider.value - steering_slider.value, 1.0), 0.0) \n",
    "        left_slider.value = left_motor\n",
    "        right_slider.value = right_motor\n",
    "\n",
    "        #run at preset speeds\n",
    "        thres = turn_thres.value #steering value must exceed threshold to initate turning\n",
    "        if steering_slider.value > thres:\n",
    "            status_a.value = str(frame_cnt) + \" turn right\"\n",
    "            robot.left_motor.value = min(robot.left_motor.value + 0.1, turn_high) *mt\n",
    "            robot.right_motor.value = max(robot.right_motor.value - 0.1, turn_low) *mt\n",
    "            stutter_spd = max(stutter_spd - 0.02, turn_low*0.9)\n",
    "            \n",
    "        elif steering_slider.value < -thres:\n",
    "            status_a.value = str(frame_cnt) + \" turn left\"\n",
    "            robot.left_motor.value = max(robot.left_motor.value - 0.1, turn_low) *mt\n",
    "            robot.right_motor.value = min(robot.right_motor.value + 0.1, turn_high) *mt\n",
    "            stutter_spd = max(stutter_spd - 0.02, turn_low*0.9)\n",
    "\n",
    "        else: \n",
    "            status_a.value = str(frame_cnt) + \" go straight\"\n",
    "            if robot.left_motor.value != robot.right_motor.value:\n",
    "                robot.left_motor.value = robot.right_motor.value = min(robot.left_motor.value,robot.right_motor.value)\n",
    "            #initial throttle then lower to coast speed    \n",
    "            robot.left_motor.value = (robot.left_motor.value - 0.01) *mt\n",
    "            robot.right_motor.value = (robot.right_motor.value - 0.01) *mt\n",
    "            if min(robot.left_motor.value,robot.right_motor.value) < fwd_low:\n",
    "                robot.left_motor.value = robot.right_motor.value = fwd_high *mt  \n",
    "            stutter_spd = min(stutter_spd + 0.01, fwd_low*0.9)\n",
    "\n",
    "        #set adjusted motor values shown in UI\n",
    "        left_adjusted.value = robot.left_motor.value\n",
    "        right_adjusted.value = robot.right_motor.value\n",
    "        #^^^^^ jetbot finished steering decision ^^^^^\n",
    "    \n",
    "    return None\n",
    "#^^^^^ jetbot image processing ^^^^^\n",
    "\n",
    "print(time.ctime(),'ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup UI - camera view, buttons, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug  3 00:26:52 2020 ok\n"
     ]
    }
   ],
   "source": [
    "#setup interface - camera view, buttons, etc\n",
    "#\n",
    "#user controlled sliders\n",
    "#1. Speed Control (speed_gain_slider): To start your JetBot increase ``speed_gain_slider`` \n",
    "#2. Steering Gain Control (steering_gain_sloder): If you see JetBot is woblling, you need to reduce ``steering_gain_slider`` till it is smooth\n",
    "#3. Steering Bias control (steering_bias_slider): If you see JetBot is biased towards extreme right or extreme left side of the track, you should control this slider till JetBot start following line or track in the center.  This accounts for motor biases as well as camera offsets\n",
    "#jetbot turns when steering value exceeds threshold (for AUTOBUS steering algo only, not in NVIDIA steering algo)\n",
    "speed_gain_slider = ipywidgets.FloatSlider(min=0, max=1, step=0.01, value=0.33, description='speed gain')\n",
    "steering_gain_slider = ipywidgets.FloatSlider(min=0, max=1, step=0.01, value=0.33, description='steering gain')\n",
    "steering_dgain_slider = ipywidgets.FloatSlider(min=0, max=1, step=0.01, value=0.33, description='steering kd')\n",
    "steering_bias_slider = ipywidgets.FloatSlider(min=-1, max=1, step=0.01, value=0, description='steering bias')\n",
    "turn_thres = ipywidgets.FloatSlider(min=0, max=1, step=0.01, value=0.25, description='steer thres') #turn threshold\n",
    "#\n",
    "#buttons\n",
    "button_layout = ipywidgets.Layout(width='80px', height='35px')\n",
    "run_button = ipywidgets.Button(description='START', button_style='success', layout=button_layout)\n",
    "run_button.on_click(lambda x: start())\n",
    "#\n",
    "stop_button = ipywidgets.Button(description='STOP', button_style='danger', layout=button_layout)\n",
    "stop_button.on_click(lambda x: stop())\n",
    "#\n",
    "camera_button = ipywidgets.Button(description='CAMERA',layout=button_layout)\n",
    "camera_button.on_click(lambda x: toggle_camera())\n",
    "#\n",
    "motor_button = ipywidgets.Button(description='MOTOR',layout=button_layout)\n",
    "motor_button.on_click(lambda x: toggle_motor())\n",
    "#\n",
    "stutter_button = ipywidgets.Button(description='STUTTER',layout=button_layout)\n",
    "stutter_button.on_click(lambda x: toggle_stutter())\n",
    "#\n",
    "#read only sliders\n",
    "x_slider = ipywidgets.FloatSlider(min=-1, max=1, step=0.01, value=0, description='x')\n",
    "y_slider = ipywidgets.FloatSlider(min=0, max=1, step=0.01, value=0, orientation='vertical', description='y')\n",
    "steering_slider = ipywidgets.FloatSlider(min=-1, max=1, step=0.01, description='steering')\n",
    "speed_slider = ipywidgets.FloatSlider(min=0, max=1, step=0.01, orientation='vertical', description='speed')\n",
    "#\n",
    "left_slider = ipywidgets.FloatSlider(min=0, max=1, step=0.01, orientation='vertical', description='L nvidia')\n",
    "right_slider = ipywidgets.FloatSlider(min=0, max=1,step=0.01, orientation='vertical', description='R nvidia')\n",
    "#\n",
    "left_adjusted = ipywidgets.FloatSlider(min=0, max=1, step=0.01, orientation='vertical', description='L adjusted')\n",
    "right_adjusted = ipywidgets.FloatSlider(min=0, max=1, step=0.01, orientation='vertical', description='R adjusted')\n",
    "#\n",
    "#The x and y sliders will display the predicted x, y values.\n",
    "#The steering slider will display our estimated steering value.  Please remember, this value isn't the actual \n",
    "#angle of the target, but simply a value that is nearly proportional.  When the actual angle is ``0``, this \n",
    "#will be zero, and it will increase / decrease with the actual angle.  \n",
    "#\n",
    "#arrange widgets in UI\n",
    "status_a = ipywidgets.Textarea(value = \"status a\")\n",
    "status_b = ipywidgets.Textarea(value = \"status b\")\n",
    "status_c = ipywidgets.Textarea(value = \"status c\")\n",
    "sliders = widgets.VBox([speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider,turn_thres])\n",
    "buttons = widgets.VBox([run_button,stop_button,camera_button,motor_button,stutter_button])\n",
    "y_sets = ipywidgets.HBox([y_slider,left_slider,right_slider,left_adjusted,right_adjusted])\n",
    "x_sets = widgets.VBox([x_slider, steering_slider, ipywidgets.Label(\"a) action\"), status_a,\n",
    "                       ipywidgets.Label(\"b) obstacle detect\"), status_b, \n",
    "                       ipywidgets.Label(\"c) color detect\"), status_c])\n",
    "#\n",
    "cam_toggle = 1 #enables/disables camera, may improve performance\n",
    "def toggle_camera():\n",
    "    global cam_toggle\n",
    "    if cam_toggle:\n",
    "        camera_link.unlink()\n",
    "        cam_toggle = 0\n",
    "    else:\n",
    "        camera_link.link()\n",
    "        cam_toggle = 1\n",
    "#\n",
    "mt = 0 #start/stop motor\n",
    "def toggle_motor():\n",
    "    global mt\n",
    "    if mt: mt = 0\n",
    "    else: mt = 1\n",
    "#       \n",
    "stutter = 0 #enable/disable stutter-steps\n",
    "def toggle_stutter():\n",
    "    global stutter\n",
    "    if stutter: stutter = 0\n",
    "    else: stutter = 1\n",
    "#\n",
    "#start processing images\n",
    "def start():\n",
    "    execute({'new': camera.value})\n",
    "    camera.observe(execute, names='value')\n",
    "#\n",
    "#stop jetbot, stop camera\n",
    "def stop():\n",
    "    time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "    camera.unobserve_all()\n",
    "    robot.stop()\n",
    "    camera.stop()\n",
    "#\n",
    "print(time.ctime(),'ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display UI and Enable Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug  3 00:27:37 2020 ok\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14f0f14eb884bb4a0f1de6ad17db06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3abf4f5bc7e452f8822601c076eba68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(FloatSlider(value=0.0, description='y', max=1.0, orientation='vertical', step=0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make camera\n",
    "try: camera.stop()\n",
    "except: print('check camera')\n",
    "fps = 10\n",
    "frame_cnt = 0 #reset frame count\n",
    "camera = Camera.instance(width=224, height=224, capture_width=224, capture_height=224, fps=fps)\n",
    "image_widget = widgets.Image(format='jpeg', width=224, height=224)\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=display_xy)\n",
    "camera.start()\n",
    "#\n",
    "if not steer_debug: #hide unused controls\n",
    "    turn_thres.layout.visibility = 'hidden'\n",
    "    left_adjusted.layout.visibility = 'hidden'\n",
    "    right_adjusted.layout.visibility = 'hidden'\n",
    "#\n",
    "#!!!!! adjustable vars !!!!!\n",
    "#!!!!! adjustable vars !!!!!\n",
    "#!!!!! adjustable vars !!!!!\n",
    "mt = 0 #0 starts jetbot without moving, hit motor button to start moving\n",
    "stutter = 0 #0 disable start stop stutter movement\n",
    "robot.left_motor.alpha = 1 #scales motor speed by this, 1 is no change\n",
    "robot.right_motor.alpha = 1 \n",
    "interval = 1.0/fps #set interval between execute's same as frame rate\n",
    "prob_block_stop = 0.8 #only stop if prob is higher than this\n",
    "leave_station_secs = 60 #jetbot will not stop at station unless it's been x secs since it's last stop\n",
    "#^^^^^ adjustable vars ^^^^^\n",
    "#^^^^^ adjustable vars ^^^^^\n",
    "#^^^^^ adjustable vars ^^^^^\n",
    "#\n",
    "#run jetbot\n",
    "execute({'new': camera.value}) #test single image run\n",
    "start() #start jetbot\n",
    "print(time.ctime(),'ok')\n",
    "#\n",
    "#show UI\n",
    "display(widgets.HBox([image_widget,sliders,buttons]))\n",
    "display(ipywidgets.HBox([y_sets,x_sets]))\n",
    "#\n",
    "#IMPORTANT: The camera must be re-initialized after it stops\n",
    "#Rerun this block to reinitialize camera\n",
    "#First run takes a few min to start up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "///////////////////////////////////////////////End of Notebook //////////////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#camera.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute({'new': camera.value})\n",
    "#camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "steer_debug = False #False = default NVIDIA steering; True = custom; don't use if NVIDIA steering code works fine\n",
    "#????? steering debug vars ????? only in effect if steer_debug = True\n",
    "if steer_debug:\n",
    "    turn_high = 0.4\n",
    "    turn_low = 0.3\n",
    "    turn_thres.value = 0.3 #only used if steer_debug=True\n",
    "    fwd_high = 0.37\n",
    "    fwd_low = 0.33\n",
    "    interval = 0.1\n",
    "if not steer_debug: #hide not used controls\n",
    "    turn_thres.layout.visibility = 'hidden'\n",
    "    left_adjusted.layout.visibility = 'hidden'\n",
    "    right_adjusted.layout.visibility = 'hidden'\n",
    "#^^^^^ steering debug vars ^^^^^ only in effect if steer_debug = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
